{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Regression Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**Team 21**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: Spain Electricity Shortfall Challenge\n",
    "\n",
    "The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "\n",
    "Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:\n",
    "\n",
    "> In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.\n",
    " \n",
    "On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge xgboostconda install -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np # for linear algebra\n",
    "import pandas as pd # Data processing, CSV file importation\n",
    "# Libraries for data preparation and model building\n",
    "##Accuracy packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from math import sqrt\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "#PARAMETER_CONSTANT = ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29bd63",
   "metadata": {},
   "source": [
    "To load your data, first ensure that the raw data and the notebook file are in the same folder on your local machine. The code below will load both the train and test data set into your notebook. If the files are not in the same folder, you will have to point to the directory in your machine or cloud location where the file is located. After loading your data, it is good practice to call up the loaded data just to verify that the data actually loaded as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data\n",
    "train_data = df_train = pd.read_csv('df_train.csv') \n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_data = df_test = pd.read_csv('df_test.csv') \n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0904d05",
   "metadata": {},
   "source": [
    "### Check the \"Shape\" of the data-sets\n",
    "Looking at the shape of both datasets, it is clear that the data has been split into 2 sets. 75% of the data is designated as the train data while 25% of the data is designated as the test data. The shape also shows that the training set has 49 columns while the test data set has only 48 coulmns. The missing column from the test set is the column that our model is to predict. We can identify that particular entity by simply identifying the entity(Column) that is missing from the test data set. From examining both datasets, that column can be identified as the load_shortfall_3hr column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cb283",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7fddb",
   "metadata": {},
   "source": [
    "### Use the \".column\" function to view the columns in your data set;\n",
    "While the .shape function has revealed the number of columns and rows which exists in your data set the, .columns function lists out the actual names of all the columns that exists in the dataframe. Find above and below the output of the column names of df_train and df_test respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9753bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558f5e9",
   "metadata": {},
   "source": [
    "### The \"describe\" function\n",
    "This function shows the summary statistics of the data. The  count feature shows the values in the represented columns that do not feature any null entries. The mean, Standards dev, minimum, maximum and quantile values are also featured in the summary stats shown by the describe funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf86e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at data statistics for df_train\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data statistics for df_test\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e021457",
   "metadata": {},
   "source": [
    "### The \"isnull\" function\n",
    "It is important to identify the columns that have null entries as null values can affect the performance of our model. The \"isnull\" function shows the number of null values that are contained in each column of the dataset. This data set is relatively clean as this function shows that only the column \"Valencia_pressure\" features null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d070cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify colunm(s) that contain null values in df_train\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify colunm(s) that contain null values in df_test\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c81c01",
   "metadata": {},
   "source": [
    "### Evaluate the correlation between the columns of the dataset\n",
    "It is neccesary to evaluate the columns to see how the values within the columns correlate. if multiple columns show strong correlation, the correlating columns will have to be removed from the data set before it is used for model creation as these columns may not add any additional functionality or advantage to the model but will only serve to increase it's size and lead to slow performance. a correlation value of 1 represents a perfect positive correlation while a value equal to -1 indicates a perfect negative correlation. The further away the value is from 1 or -1, the weaker the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7793b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate correlation for df_train\n",
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate correlation for df_test\n",
    "df_test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at feature distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891bd12",
   "metadata": {},
   "source": [
    "### Merge the data sets\n",
    "Observe that so far, we have had to repeat each step we take for the Test and the Train data set. We can simplify the work done in EDA, Data cleaning and pre-processing by merging the Test and Train Data-sets. When you merge your data, any action carried out on one set affects the other data set as well, so you can carry out the actions neccesary for data cleaning only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ee8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge the test and train data set to simplify your work \n",
    "# df_train = pd.concat([df_train, df_test])\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deac069",
   "metadata": {},
   "source": [
    "### Check for null Values in df_train\n",
    "What we expect to see is the sum of the null values contained in the df_train and the df_test data frames. We also expect the load_shortfall_3hr data column from the entire df_train data set to return null values. This is consistent with what we get from the merged data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ceea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef81c7",
   "metadata": {},
   "source": [
    "### Check the tail of the merged dataset\n",
    "Now let us check out the tail of df_train. We want to check to confirm that the data represented at the tail is consitent with the data we get from the df_test data set. Recall that the lower 25% of df_train is the df_test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec75c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd2cd7",
   "metadata": {},
   "source": [
    "### Fix null entries (train)\n",
    "Looking at the  dataset, everything seems to be in order except the \"Valencia_pressure\" data set which records 2,522 null entries. Note that the load_shortfall_3h has null entries because it is the value that is to be predicted by the model. After merging the train and  the test data-sets to simplify the work of pre-processing and data cleaning, any action carried out on the merged data set affects bothe the df_train and the df_test data sets as they are both one single dataframe now.So, you can carry out the actions neccesary for data cleaning only once on the merged data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af396c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_train['Valencia_pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mode')\n",
    "print(df_train['Valencia_pressure'].mode())\n",
    "print('Mean')\n",
    "print(df_train['Valencia_pressure'].mean())\n",
    "print('Median')\n",
    "print(df_train['Valencia_pressure'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ec226",
   "metadata": {},
   "source": [
    "### Fix null entries (test)\n",
    "Looking at the  dataset, everything seems to be in order except the \"Valencia_pressure\" data set which records 2,522 null entries. Note that the load_shortfall_3h has null entries because it is the value that is to be predicted by the model. After merging the train and  the test data-sets to simplify the work of pre-processing and data cleaning, any action carried out on the merged data set affects bothe the df_train and the df_test data sets as they are both one single dataframe now.So, you can carry out the actions neccesary for data cleaning only once on the merged data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f52a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mode')\n",
    "print(df_test['Valencia_pressure'].mode())\n",
    "print('Mean')\n",
    "print(df_test['Valencia_pressure'].mean())\n",
    "print('Median')\n",
    "print(df_test['Valencia_pressure'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0379a",
   "metadata": {},
   "source": [
    "### Choosing the value to replace null values\n",
    "To fix the null value problem, you can choose to either remove the entries with these nulls from your data set or fill in the  the null values by replacing the nulls with the mean, median or mode. It wouldn't really make too much of a difference which entry you choose to go with as these features are actually quite simmilar looking at the measures of centralization of the \"Valencia_pressure\" from the data set and the box-plot shown above. For the purpose of this model, we shalll go with the last option and fill out the Null values in \"Valencia pressure\" with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8923ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the cleaned data\n",
    "df_train = df_train\n",
    "df_train['Valencia_pressure'] = df_train['Valencia_pressure'].fillna(df_train['Valencia_pressure'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da137d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the cleaned data\n",
    "df_test = df_test\n",
    "df_test['Valencia_pressure'] = df_test['Valencia_pressure'].fillna(df_test['Valencia_pressure'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48986885",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e793b3",
   "metadata": {},
   "source": [
    "### Check data types\n",
    "Now that we have taken care of the null values, we can check to identify the data types contained in the dataset. Machine learning models only work with numeric data, which means the data types for the models must be floats or integers in order to get the best predictions out of the models built from our data.The code bellow reveals the data types of the data contained in our data set. Note that the \"time\" data, the \"Valencia_wind_deg\" data as well as the \"Seville_pressure\" are all object data (also known as strings). These have to be converted to floats or integers for the model to be able to make use of them as inputs. Like the null values, you can handle this problem by simply dropping the colunms. This is not recommended as everytime you drop data, you are loosing pottentially valuable information that may be very useful for your model building efforts. A more beneficial approach will be to process this data by Transforming it to numeric form or encoding it to a form that the model can utilize. 3 non-numeric objects are observed from the df_train data set. they are \"time\", \"Valencia_wind_deg\" and \"Seville_pressure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc31c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603ecef",
   "metadata": {},
   "source": [
    "### The time column\n",
    "Let us take a more thorough look at the object elements in our data frame. We will start with the time column. We need to take that column and map it into a date_time format which is the form that is usable by models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209677f5",
   "metadata": {},
   "source": [
    "To convert to datetime, you use the .to_date_time function in the pandas library. Notice that when you output the code to convert the object to .to_date_time, it looks at first glance as though nothing has changed. When you observe the last row though, the data type (dtype) that was represented as an \"object\" is now represented as \"datetime64[ns]\". This format is readable by some machine learning models while objects can not be read by any model. Linear regression models fall among the models that cannot read even this time format though. visit https://www.analyticsvidhya.com/blog/2020/05/datetime-variables-python-pandas/ to link to a resource with detailed instruction on how to adapt the date_time data for use by a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['time'] = pd.to_datetime(df_train['time'])\n",
    "df_train.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['time'] = pd.to_datetime(df_test['time'])\n",
    "df_test.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c804de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18242a",
   "metadata": {},
   "source": [
    "From the dataset it seems we will rely hevily on the weather. We can in turn extract the week, time of day and year from the time column as they may be usefull predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f874b5d",
   "metadata": {},
   "source": [
    "### The week column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ff038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['week'] = df_train['time'].apply(lambda x: x.isocalendar()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb75130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['hour'] = df_train['time'].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['hour'] = df_test['time'].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def add_datepart(df, fldname, drop=True, time=False):\n",
    "    \"Helper function that adds columns relevant to a date.\"\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169761c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(df_train, \"time\", drop=True, time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(df_test, \"time\", drop=True, time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['week'] = df_test['time'].apply(lambda x: x.isocalendar()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['week'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbdae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['week'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee067d",
   "metadata": {},
   "source": [
    "### The time_of_day column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72495817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['hour'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e14aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['hour'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff812280",
   "metadata": {},
   "source": [
    "### The year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['year'] = df_train['time'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['year'] = df_test['time'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15266eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814bc5d5",
   "metadata": {},
   "source": [
    "### The Quarter column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdccb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['quarter'] = df_train['time'].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dea626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['quarter'] = df_test['time'].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35787bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['quarter'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c50936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['quarter'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1f7e0",
   "metadata": {},
   "source": [
    "### The Valencia_wind_deg Column\n",
    "The next object in the dataset is the \"Valencia_wind_deg\". This is recorded as levels which are denoted by the string \"level\" followed by a number which describes that particular level. To encode this, we can simply extract the number from the column that identifies that level. This line of code can help us to achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac91cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Valencia_wind_deg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab62c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Valencia_wind_deg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Valencia_wind_deg'] = df_train['Valencia_wind_deg'].str.extract('(\\d+)')\n",
    "df_train['Valencia_wind_deg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Valencia_wind_deg'] = df_test['Valencia_wind_deg'].str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27592d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Valencia_wind_deg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add7351",
   "metadata": {},
   "source": [
    "As you can see above, the data has been reduced to a number without the string \"level\" to define it but there is still a problem. The data type is still an object. we can convert the object to numeric form by using the pandas numeric function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Valencia_wind_deg'] = pd.to_numeric(df_train['Valencia_wind_deg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaebf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Valencia_wind_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f781fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Valencia_wind_deg'] = pd.to_numeric(df_test['Valencia_wind_deg'])\n",
    "df_test.Valencia_wind_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb689b0",
   "metadata": {},
   "source": [
    "Repeat the process for Seville_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7093fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Seville_pressure'] = df_train['Seville_pressure'].str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Seville_pressure'] = df_test['Seville_pressure'].str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Seville_pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0451e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Seville_pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eea17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Seville_pressure'] = pd.to_numeric(df_train['Seville_pressure'])\n",
    "df_train.Seville_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Seville_pressure'] = pd.to_numeric(df_test['Seville_pressure'])\n",
    "df_test.Seville_pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb093be",
   "metadata": {},
   "source": [
    "### Variable Selection by Correlation and Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386f109",
   "metadata": {},
   "source": [
    "The code below will create a new DataFrame and store the correlation coefficents and p-values in that DataFrame for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between predictor variables and the response variable\n",
    "corrs = df_train[df_train['load_shortfall_3h'].notnull()].corr()['load_shortfall_3h'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Build a dictionary of correlation coefficients and p-values\n",
    "dict_cp = {}\n",
    "\n",
    "column_titles = [col for col in corrs.index if col!= 'load_shortfall_3h']\n",
    "for col in column_titles:\n",
    "    p_val = round(pearsonr(df_train[df_train['load_shortfall_3h'].notnull()][col], df_train[df_train['load_shortfall_3h'].notnull()]['load_shortfall_3h'])[1],6)\n",
    "    dict_cp[col] = {'Correlation_Coefficient':corrs[col],\n",
    "                    'P_Value':p_val}\n",
    "    \n",
    "df_cp = pd.DataFrame(dict_cp).T\n",
    "df_cp_sorted = df_cp.sort_values('P_Value')\n",
    "df_cp_sorted[df_cp_sorted['P_Value']<0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955dd2d0",
   "metadata": {},
   "source": [
    "All the features seem to be statistically significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef9a07",
   "metadata": {},
   "source": [
    "However, we also need to look for predictor variable pairs which have a high correlation with each other to avoid autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe389e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15));\n",
    "ax = fig.add_subplot(111);\n",
    "plot_corr(df_train.corr(), xnames = df_train.corr().columns, ax = ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4655d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = [col for col in df_train.columns if col != 'load_shortfall_3h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ba53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCorr = df_train[X_names].corr()\n",
    "filteredDf = dfCorr[((dfCorr >= .5) | (dfCorr <= -.5)) & (dfCorr !=1.000)]\n",
    "plt.figure(figsize=(30,10))\n",
    "sns.heatmap(filteredDf, annot=True, cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237ff1c",
   "metadata": {},
   "source": [
    "From the plot we can see that there are quite a number of variables with high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94191dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the correlation matrix\n",
    "corr = df_train[X_names].corr()\n",
    "\n",
    "# mask away the lower triangle and diagonal\n",
    "mask = np.triu(np.ones_like(corr),1) == 1\n",
    "\n",
    "# get the upper triangle (excluding diagonal) by masking and stack:\n",
    "corr = corr.where(mask).stack()\n",
    "\n",
    "# 10 largest by absolute values\n",
    "max10 = corr.abs().nlargest(40)\n",
    "max10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad70f7b",
   "metadata": {},
   "source": [
    "### Dropping the noise\n",
    "We need to drop columns that are not useful to the model. For now that would be the \"Unnamed:0\" and the \"time\" columns. Note that you can opt to add the time column to your model but that would require encoding the time data untill it is in a form that is usable by the model. We see by running the .head function that the unwanted columns have indeed been dropped from our df. Your data is now clean and ready for use in model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cc471",
   "metadata": {},
   "source": [
    "### 2. Feature Selection- With Correlation\n",
    "In this step we will be removing the features which are highly correlated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = [col for col in df_train.columns if col != 'load_shortfall_3h']\n",
    "X = df_train[X_names]\n",
    "y = df_train['load_shortfall_3h']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6762479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(15,15))\n",
    "cor = X_train.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the following function we can select highly correlated features\n",
    "# it will remove the first feature that is correlated with anything other feature\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if (corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = correlation(X_train, 0.85)\n",
    "len(set(corr_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32499c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.drop(corr_features,axis=1)\n",
    "# X_test.drop(corr_features,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2195c0c",
   "metadata": {},
   "source": [
    "df_train = df_train.drop(\n",
    "    ['Unnamed: 0' , 'time', 'year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.drop(\n",
    "#     ['Unnamed: 0' , 'Madrid_temp_max', 'Bilbao_temp_max', 'Barcelona_temp_max', 'Valencia_temp_max', 'Seville_temp_max', \n",
    "#     'Madrid_temp_min', 'Bilbao_temp_min', 'Barcelona_temp_min', 'Valencia_temp_min', 'Seville_temp_min'], axis = 1)\n",
    "\n",
    "df_train = df_train.drop(['Unnamed: 0' ,\"Barcelona_temp\",\n",
    " 'Barcelona_temp_min',\n",
    " 'Bilbao_temp',\n",
    " 'Bilbao_temp_max',\n",
    " 'Madrid_temp',\n",
    " 'Madrid_temp_min',\n",
    " 'Seville_temp_min',\n",
    " 'Valencia_temp',\n",
    " 'Valencia_temp_min',\n",
    " 'timeDayofyear',\n",
    " 'timeElapsed',\n",
    " 'timeWeek',\n",
    " 'timeYear'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807886ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df_test.drop(\n",
    "#     ['Unnamed: 0' , 'Madrid_temp_max', 'Bilbao_temp_max', 'Barcelona_temp_max', 'Valencia_temp_max', 'Seville_temp_max', \n",
    "#     'Madrid_temp_min', 'Bilbao_temp_min', 'Barcelona_temp_min', 'Valencia_temp_min', 'Seville_temp_min'], axis = 1)\n",
    "df_test = df_test.drop(['Unnamed: 0' ,\"Barcelona_temp\",\n",
    " 'Barcelona_temp_min',\n",
    " 'Bilbao_temp',\n",
    " 'Bilbao_temp_max',\n",
    " 'Madrid_temp',\n",
    " 'Madrid_temp_min',\n",
    " 'Seville_temp_min',\n",
    " 'Valencia_temp',\n",
    " 'Valencia_temp_min',\n",
    " 'timeDayofyear',\n",
    " 'timeElapsed',\n",
    " 'timeWeek',\n",
    " 'timeYear'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e38027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c8412",
   "metadata": {},
   "source": [
    "### Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952262a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Applying it on the data set\n",
    "# clean_dataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the response\n",
    "X_names = [col for col in df_train.columns if col != 'load_shortfall_3h']\n",
    "X = df_train[X_names]\n",
    "y = df_train['load_shortfall_3h']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Lasso\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# lasso=Lasso()\n",
    "# parameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,0.01,1,5,10,20,30,35,40,45,50,55,100]}\n",
    "# lasso_regressor=GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "\n",
    "# lasso_regressor.fit(X,y)\n",
    "# print(lasso_regressor.best_params_)\n",
    "# print(lasso_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02510177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardization object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save standardized features into new variable\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_names)\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a15c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=1,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6580872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the correlation matrix\n",
    "corr = X_scaled.corr()\n",
    "\n",
    "# mask away the lower triangle and diagonal\n",
    "mask = np.triu(np.ones_like(corr),1) == 1\n",
    "\n",
    "# get the upper triangle (excluding diagonal) by masking and stack:\n",
    "corr = corr.where(mask).stack()\n",
    "\n",
    "# 10 largest by absolute values\n",
    "max10 = corr.abs().nlargest(60)\n",
    "max10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810ea77",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "Recall that we merged our train and test data sets in order to make cleaning and preprocessing easier. Now that we are ready to build our model, it is time to split this back into the distinct data sets. The code below splits our data into the train and test data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "y = df_train[:len(df_train)][['load_shortfall_3h']]\n",
    "x = X_scaled[:len(df_train)]\n",
    "\n",
    "x_train = df_train[:len(df_train)].drop('load_shortfall_3h',axis=1)\n",
    "\n",
    "#Ignore for now. Will be used when model is built and ready to be tested\n",
    "x_test = df_train[len(df_train):].drop('load_shortfall_3h',axis=1) \n",
    "# x_test = df_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f0f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa00dc",
   "metadata": {},
   "source": [
    "### Load Your Model\n",
    "you are now ready to load up your model. For the purpose of this work, we shall be using the most basic type of model, a linear regression model. Note that other models will significantly improve your model performance and you are encouraged to try out other models to see how they perform compared to this one so as to choose the model that performs best. Note that a quick google search will return other models which you may use and applying the model to your data set is as easy as replacing the code in the cell below with the code string that instantiates the model and tweaking the hyperparameters to your taste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(XGBRegressor(booster=\"gbtree\",eta=0.2,eval_metric= \"rmse\", n_estimators=1000))\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=300))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=300))\n",
    "\treturn models\n",
    " \n",
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_y_pred = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X.iloc[train_ix], X.iloc[test_ix]\n",
    "\t\ttrain_y, test_y = y.iloc[train_ix], y.iloc[test_ix]\n",
    "\t\tmeta_y.extend(test_y.values.ravel())\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y.values.ravel())\n",
    "\t\t\ty_pred = model.predict(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_y_pred.append(y_pred.reshape(len(y_pred),1))\n",
    "\t\t# store fold y_pred as columns\n",
    "\t\tmeta_X.append(hstack(fold_y_pred))\n",
    "\treturn vstack(meta_X), asarray(meta_y)\n",
    " \n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y.values.ravel())\n",
    " \n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = Lasso(alpha=1e-15)\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    " \n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\ty_pred = model.predict(X)\n",
    "\t\tmse = mean_squared_error(y, y_pred)\n",
    "\t\tprint('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))\n",
    " \n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\ty_pred = model.predict(X)\n",
    "\t\tmeta_X.append(y_pred.reshape(len(y_pred),1))\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e344e2e",
   "metadata": {},
   "source": [
    "### Training your model\n",
    "The test size represents the proportion of the data that is intended for use as the \"Test data\", Where test size is set at 0.25, you are simply telling the algorithm to use 75% of the data to train the model and 25% to test the model. This value can be set at any figure that the model builder chooses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60012590",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.20, random_state=1)\n",
    "print('Train', x_train.shape, y_train.shape, 'Test', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f954c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get models\n",
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(x_train, y_train, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def2d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit base models\n",
    "fit_base_models(x_train, y_train, models)\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24440d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate base models\n",
    "evaluate_models(x_test, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98557ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate meta model\n",
    "y_pred = super_learner_predictions(x_test, models, meta_model)\n",
    "print('Super Learner: RMSE %.3f' % (sqrt(mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea499ce5",
   "metadata": {},
   "source": [
    "### The Root Mean squared Error (RMSE)\n",
    "The root mean squared error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the actual values observed. It is a very useful tool in telling how well your model predicted the values in the test data set. Below is a function that calculates the returns the average RMSE of the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb75193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_test, y_predict):\n",
    "    return np.sqrt(mean_squared_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a90e1",
   "metadata": {},
   "source": [
    "This means that on the average, the predictions of your model deviated from the actual values by about \"4858.875754272154\". You can better appreciate the implications of this value when you compare it to the mean y_train value, which are the actual values used in training the model. This RMSE value means that a load_shortfall_3h value that is actually 10,000 could possibly have been predicted by your model to be 10,000 +- 4858.875754272154, which implies that your models prediction can be lower than 6000 and can be higher than 14,000. Not too good a performance you might say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1451c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9600ea",
   "metadata": {},
   "source": [
    "### The r squared score\n",
    "Another metric that is useful in assessing model performance is the r squared score. This score is a measure of the percentage of accuracy of your models predictions. We import this metric from sklearn with the code below. The r2_score of our model reveals that our model returns a correct prediction only 13.4% of the time. A rather poor performance again but bear in mind that thios is aa very basic model that has not been optimized in most ways possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaee681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is another model evaluation tool that tells you how well your mode.l performs\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0139c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5072f5df",
   "metadata": {},
   "source": [
    "### How to Improve your Model and Optimize performance\n",
    "consider taking these steps to generate better models and enhance the performance of your generated models. \n",
    "* Better Model: This is  just a basic linear regression model. Use google to find other models\n",
    "and try them out to see how this can make your model performance ratings to improve.\n",
    "\n",
    "* Better Features: As you know, we dropped the date_time feature which just might be a very helpful\n",
    "feature to improve our model performance. Try to get this particular matrix back into your model by perfecting the neccesary steps to encode the \"time\" data until into a form usable by your model. It will also be helpful to drop highly correlated features from the model.\n",
    "\n",
    "* Hyper Parameter tuning: All models come with default features that can be edited in relation to the data set\n",
    "to improve model performance. Be careful in modifying hyper parameters though as this may also have a negative \n",
    "impact on the performance of your model.  \n",
    "\n",
    "* Cross validation: Use cross validation to improve model performance(refer to the cross validation train)\n",
    "\n",
    "***(Please note that you can and should improve your notebook by including extensive relevant visualizations as a tool for your EDA. Also, this list of recommendations is by no means exhaustive. You at liberty to research and apply other strategies to improve the performance of your model and make your presentation better.)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a863a1",
   "metadata": {},
   "source": [
    "### Making a Kaggle submission\n",
    "After you are done creating your model, you can make a kaggle submission from your models results by following this steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc92891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designate the dataframes to be used for model training and testing\n",
    "x_train = df_train[:len(df_train)].drop('load_shortfall_3h',axis=1)\n",
    "x_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c803da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit your models and make your predictions\n",
    "\n",
    "# fit base models\n",
    "fit_base_models(x_train, y, models)\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "# evaluate meta model\n",
    "preds = super_learner_predictions(x_test, models, meta_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm that your predictions have actually been generated\n",
    "daf=pd.DataFrame(preds, columns=['load_shortfall_3h'])\n",
    "daf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_test = pd.read_csv('df_test.csv') \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code to generate a .csv file of your submission\n",
    "output = pd.DataFrame({'time':df_test['time']})\n",
    "submission2 = output.join(daf)\n",
    "submission2.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb3cde",
   "metadata": {},
   "source": [
    "### The entry submission process\n",
    "To run a submission on kaggle, all of the steps involved in the creation of your model as stated above should have been completed on Kaggle. Once you are done with the process and your submission file has been generated and you are happy with the results of the process, locate the save version button on the top right corner of the kaggle page. click on save & run all (commit) button that shows up at the center of your screen and finally click on save to save the output. When you click on the code button, you will find a file called out-put. Clicking on this file will reveal your .csv file that has been submitted to kaggle. You can now submit this file as an entry on kaggle or download it if you prefer to.\n",
    "\n",
    "Alternatively, you can submit by downloading the .csv file, go to the leaderboard, click on submit and upload your submission file from wherever it is saved on you local machine. Your entry will be evaluated immediately and you will be told your score.\n",
    "\n",
    "***Note that you can run multiple submissions, as many times as you wish before the official closing date of the submission. Each time your entry will be evaluated and told\n",
    "what your score is.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d073e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one or more ML models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model and motivate why it is the best choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss chosen methods logic"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "320c1f05b41b6296d6cdeadbc8f37198b22e160db062b16d8b8cc9d95c25d782"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
